Summary & Highlights

In this module, you learned that:

    Spark is an open source in-memory application framework for distributed data processing and iterative analysis on massive data volumes. Both distributed systems and Apache Spark are inherently scalable and fault tolerant. Apache Spark solves the problems encountered with MapReduce by keeping a substantial portion of the data required in-memory, avoiding expensive and time-consuming disk I/O.

    Functional programming follows a declarative programming model that emphasizes “what” instead of “how to” and uses expressions.

    Lambda functions or operators are anonymous functions that enable functional programming. Spark parallelizes computations using the lambda calculus and all functional Spark programs are inherently parallel.

    Resilient distributed datasets, or RDDs, are Spark’s primary data abstraction consisting of a fault-tolerant collection of elements partitioned across the nodes of the cluster, capable of accepting parallel operations.You can create an RDD using an external or local Hadoop-supported file, from a collection, or from another RDD. RDDs are immutable and always recoverable, providing resilience in Apache Spark RDDs can persist or cache datasets in memory across operations, which speeds iterative operations in Spark.

    Apache Spark architecture consists of components data, compute input, and management. The fault-tolerant Spark Core base engine performs large-scale Big Data worthy parallel and distributed data processing jobs, manages memory, schedules tasks, and houses APIs that define RDDs.

    Spark SQL provides a programming abstraction called DataFrames and can also act as a distributed SQL query engine. Spark DataFrames are conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations.


    In this module you learned that:

    To connect to the Apache Spark user interface web server, start your application and connect to the application UI using the
    following URL:  http://<driver-node>:4040

    The Spark application UI centralizes critical information, including status information into the Jobs, Stages, Storage, Environment and Executors tabbed regions. You can quickly identify failures, then drill down to the lowest levels of the application to discover their root causes. If the application runs SQL queries, select the SQL tab and the Description hyperlink to display the query’s details.

    The Spark application workflow includes jobs created by the Spark Context in the driver program, jobs in progress running as tasks in the executors, and completed jobs transferring results back to the driver or writing to disk.

    Common reasons for application failure on a cluster include user code, system and application configurations, missing dependencies, improper resource allocation, and network communications. Application log files, located in the Spark installation directory, will often show the complete details of a failure.

    User code specific errors include syntax, serialization, data validation. Related errors can happen outside the code If a task fails due to an error, Spark can attempt to rerun tasks for a set number of retries. If all attempts to run a task fail, Spark reports an error to the driver and terminates the application. The cause of an application failure can usually be found in the driver event log. 

    Spark enables configurable memory for executor and driver processes. Executor memory and Storage memory share a region that can be tuned. 

    Setting data persistence by caching data is one technique used to improve application performance.

    The following code example illustrates configuration of executor memory on submit for a Spark Standalone cluster:

    $ ./bin/spark-submit \
    --class org.apache.spark.examples.SparkPi \
    --master
    spark://<spark-master-URL>:7077 \
    --executor-memory 10G \
    /path/to/examples.jar \1000

    The following code example illustrates setting Spark Standalone worker memory and core parameters:

    # Start standalone worker with MAX 10Gb memory, 8 cores
    $
    ./sbin/start-worker.sh \  
    spark://<spark-master-URL> \
    –-memory 10G –-cores 8

    Spark assigns processor cores to driver and executor processes during application processing. Executors process tasks in parallel according to the number of cores available or as assigned by the application.  

    You can apply the argument ‘--executor-cores 8 \’ to set executor cores on submit per executor. This example specifies eight cores.

    You can specify the executor cores for a Spark standalone cluster for the application using the argument ‘‘--total-executor-cores 50’ followed by the number of cores for the application. This example specifies 50 cores.

    When starting a worker manually in a Spark standalone cluster, you can specify the number of cores the application uses by using the argument ‘--cores‘ followed by the number of cores. Spark’s default behavior is to use all available cores. 
